# Issue Triaging Agent - Project Plan

## üéØ Project Overview

An intelligent multi-agent system that automatically triages GitHub issues by classifying, prioritizing, routing, and responding to issues in real-time. Reduces maintainer burden by 60-80% while improving issue resolution speed.

**Target Users:** Open source maintainers, engineering teams, DevOps organizations

**Core Value Proposition:** Turn chaotic issue queues into organized, actionable workflows with AI-powered automation.

---

## üìä Success Metrics

### Technical Metrics
- Classification accuracy: >85%
- Duplicate detection precision: >80%
- Response time: <30 seconds per issue
- System uptime: >99%

### Business Metrics
- Time saved per issue: 5-10 minutes
- Reduction in manual triage: 60-80%
- Faster first response: 90% reduction
- User satisfaction: >4/5 stars

### Portfolio Metrics
- Production-ready demo
- Quantifiable impact stories
- Multi-agent architecture showcase
- MLOps best practices

---

## üóìÔ∏è Development Phases

### **Phase 1: MVP (Weeks 1-2)** ‚≠ê START HERE

**Goal:** Basic working prototype that demonstrates core value

#### Features
- ‚úÖ GitHub webhook integration
- ‚úÖ Issue classification (bug, feature, question, docs)
- ‚úÖ Auto-labeling
- ‚úÖ Basic routing to team members
- ‚úÖ Simple auto-reply templates

#### Tech Stack
- **Backend:** FastAPI
- **Agents:** LangChain
- **LLM:** GPT-3.5-turbo (cost-effective for MVP)
- **Database:** SQLite (simple start)
- **Deployment:** Docker + Railway/Render

#### Deliverables
```
issue-triaging-agent/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classifier_agent.py      # Issue classification
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routing_agent.py         # Team assignment
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ response_agent.py        # Auto-reply generation
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ webhooks.py              # GitHub webhook handler
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py               # Pydantic models
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ github_client.py         # GitHub API wrapper
‚îÇ       ‚îî‚îÄ‚îÄ config.py                # Configuration
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_agents.py
‚îÇ   ‚îî‚îÄ‚îÄ test_webhooks.py
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

#### Week 1 Tasks
- [ ] Set up project structure
- [ ] Implement GitHub webhook receiver
- [ ] Build classification agent (LangChain + GPT-3.5)
- [ ] Create basic routing logic
- [ ] Test with sample issues

#### Week 2 Tasks
- [ ] Implement auto-labeling via GitHub API
- [ ] Build response generation agent
- [ ] Add error handling & logging
- [ ] Write unit tests
- [ ] Deploy to Railway/Render
- [ ] Test with real GitHub repo

---

### **Phase 2: Intelligence Layer (Weeks 3-4)**

**Goal:** Add smart features that demonstrate ML expertise

#### Features
- ‚úÖ Duplicate detection using embeddings
- ‚úÖ Similarity search (link related issues)
- ‚úÖ Priority prediction based on historical data
- ‚úÖ Context extraction (error messages, stack traces)

#### Tech Stack Additions
- **Vector DB:** ChromaDB (lightweight, embedded)
- **Embeddings:** OpenAI text-embedding-3-small
- **Database:** Upgrade to PostgreSQL

#### New Components
```
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ similarity_agent.py      # Duplicate detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ priority_agent.py        # Priority prediction
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py          # ChromaDB interface
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embedding_generator.py   # OpenAI embeddings
‚îÇ   ‚îî‚îÄ‚îÄ ml/
‚îÇ       ‚îî‚îÄ‚îÄ priority_model.py        # Priority classifier
```

#### Tasks
- [ ] Implement embedding pipeline for all issues
- [ ] Build similarity search (cosine similarity)
- [ ] Train simple priority classifier (Random Forest/XGBoost)
- [ ] Add context extraction (regex + NER)
- [ ] Create evaluation metrics dashboard

---

### **Phase 3: Production Features (Weeks 5-6)**

**Goal:** Make it production-ready and scalable

#### Features
- ‚úÖ Async processing with message queue
- ‚úÖ Rate limiting & retry logic
- ‚úÖ Comprehensive logging & monitoring
- ‚úÖ Admin dashboard (Streamlit)
- ‚úÖ Configuration management

#### Tech Stack Additions
- **Queue:** Redis + Celery (async task processing)
- **Monitoring:** Prometheus + Grafana
- **Logging:** Structured logging (loguru)
- **Dashboard:** Streamlit

#### Architecture Upgrade
```
GitHub Webhook ‚Üí FastAPI ‚Üí Redis Queue ‚Üí Celery Workers ‚Üí Agents
                                              ‚Üì
                                    PostgreSQL + ChromaDB
                                              ‚Üì
                                    Metrics ‚Üí Prometheus ‚Üí Grafana
```

#### Tasks
- [ ] Implement Celery task queue
- [ ] Add Redis for caching & queue
- [ ] Build monitoring dashboard
- [ ] Add rate limiting (GitHub API limits)
- [ ] Implement retry logic with exponential backoff
- [ ] Create admin Streamlit dashboard

---

### **Phase 4: Advanced Features (Weeks 7-8)**

**Goal:** Showcase cutting-edge ML/AI capabilities

#### Features
- ‚úÖ Multi-agent coordination (CrewAI)
- ‚úÖ Knowledge base integration (RAG)
- ‚úÖ Sentiment analysis
- ‚úÖ Automated fix suggestions (for simple bugs)

#### Tech Stack Additions
- **Agents:** Migrate to CrewAI for better orchestration
- **RAG:** LlamaIndex for documentation search
- **Sentiment:** Transformers (DistilBERT)

#### New Components
```
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ crew_orchestrator.py     # CrewAI coordination
‚îÇ   ‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ knowledge_base.py        # LlamaIndex RAG
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ doc_indexer.py           # Documentation indexing
‚îÇ   ‚îî‚îÄ‚îÄ ml/
‚îÇ       ‚îú‚îÄ‚îÄ sentiment_analyzer.py    # User sentiment
‚îÇ       ‚îî‚îÄ‚îÄ code_suggester.py        # Simple fix suggestions
```

#### Tasks
- [ ] Migrate to CrewAI multi-agent system
- [ ] Index project documentation with LlamaIndex
- [ ] Implement RAG for context-aware responses
- [ ] Add sentiment analysis for priority escalation
- [ ] Build simple code fix suggester (pattern matching)

---

## üèóÔ∏è Technical Architecture

### System Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        GitHub Issues                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ Webhook
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      FastAPI Server                          ‚îÇ
‚îÇ  ‚îú‚îÄ Webhook Handler                                          ‚îÇ
‚îÇ  ‚îú‚îÄ Authentication & Validation                              ‚îÇ
‚îÇ  ‚îî‚îÄ Rate Limiting                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Redis Task Queue                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Celery Workers (Agents)                    ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ Classification   ‚îÇ  ‚îÇ  Similarity      ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ Agent            ‚îÇ  ‚îÇ  Search Agent    ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ (GPT-4)          ‚îÇ  ‚îÇ  (Embeddings)    ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ Routing Agent    ‚îÇ  ‚îÇ  Response Agent  ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ (Rule + LLM)     ‚îÇ  ‚îÇ  (GPT-4)         ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ Priority Agent   ‚îÇ  ‚îÇ  Knowledge Base  ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ (ML Classifier)  ‚îÇ  ‚îÇ  RAG (LlamaIndex)‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PostgreSQL    ‚îÇ              ‚îÇ    ChromaDB     ‚îÇ
‚îÇ                 ‚îÇ              ‚îÇ                 ‚îÇ
‚îÇ - Issue history ‚îÇ              ‚îÇ - Embeddings    ‚îÇ
‚îÇ - Metrics       ‚îÇ              ‚îÇ - Similarity    ‚îÇ
‚îÇ - User data     ‚îÇ              ‚îÇ   search        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                                  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       GitHub API                             ‚îÇ
‚îÇ  ‚îú‚îÄ Apply labels                                             ‚îÇ
‚îÇ  ‚îú‚îÄ Assign users                                             ‚îÇ
‚îÇ  ‚îú‚îÄ Post comments                                            ‚îÇ
‚îÇ  ‚îî‚îÄ Update metadata                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

```
1. Issue Created ‚Üí Webhook triggered
2. FastAPI validates & queues task
3. Celery worker picks up task
4. Agents process in parallel:
   - Classifier: Determines issue type
   - Similarity: Finds duplicates
   - Priority: Calculates urgency
   - Routing: Assigns team/person
   - Response: Generates reply
5. Results aggregated
6. GitHub API called with updates
7. Metrics logged to PostgreSQL
8. User notified
```

---

## üõ†Ô∏è Technology Decisions

### Core Stack

| Component | Choice | Reasoning |
|-----------|--------|-----------|
| **Backend** | FastAPI | Async support, fast, great for webhooks |
| **Agents** | LangChain ‚Üí CrewAI | Start simple, upgrade to multi-agent |
| **LLM** | GPT-3.5 ‚Üí GPT-4 | Cost-effective MVP, upgrade for accuracy |
| **Vector DB** | ChromaDB | Lightweight, embedded, perfect for MVP |
| **Database** | SQLite ‚Üí PostgreSQL | Easy start, production-ready upgrade |
| **Queue** | Celery + Redis | Industry standard, reliable |
| **Monitoring** | Prometheus + Grafana | Open-source, comprehensive |
| **Deployment** | Docker + Railway | Simple, cost-effective |

### Alternative Considerations

**If cost is major concern:**
- Use Llama 3 8B (via Ollama locally)
- Use Groq API (fast, generous free tier)

**If scaling is priority:**
- Use AWS SQS instead of Celery
- Deploy on ECS/Lambda

**If wanting to showcase fine-tuning:**
- Fine-tune Llama 3 8B on issue classification
- Use LoRA/QLoRA for efficiency

---

## üìù Key Features Detail

### 1. Issue Classification

**Input:** Issue title + body
**Output:** Category (bug, feature, question, docs, security)

**Approach:**
```python
# Prompt template
system_prompt = """
You are an expert at classifying GitHub issues.
Categories: bug, feature_request, question, documentation, security

Analyze the issue and respond with ONE category.
"""

# Few-shot examples
examples = [
    {"title": "App crashes on startup", "label": "bug"},
    {"title": "Add dark mode", "label": "feature_request"},
    {"title": "How do I configure X?", "label": "question"}
]
```

**Metrics:**
- Accuracy on labeled test set
- Confusion matrix
- F1-score per category

### 2. Duplicate Detection

**Input:** New issue embedding
**Output:** Similar issues (cosine similarity > 0.85)

**Approach:**
```python
# Generate embedding
embedding = openai.embeddings.create(
    model="text-embedding-3-small",
    input=f"{issue.title} {issue.body}"
)

# Search ChromaDB
results = vector_store.query(
    query_embeddings=[embedding],
    n_results=5,
    where={"status": {"$ne": "closed"}}  # Only open issues
)

# Filter by threshold
duplicates = [r for r in results if r.similarity > 0.85]
```

### 3. Smart Routing

**Input:** Issue category + historical data
**Output:** Assigned team/person

**Approach:**
```python
# Rule-based routing
routing_rules = {
    "bug": {
        "authentication": ["@security-team", "@backend-team"],
        "ui": ["@frontend-team"],
        "performance": ["@backend-team", "@devops"]
    },
    "feature_request": {
        "default": ["@product-team"]
    }
}

# ML-enhanced (Phase 2)
# Train classifier on historical assignments
# Features: issue text, labels, component mentions
```

### 4. Response Generation

**Input:** Issue + context
**Output:** Helpful auto-reply

**Templates:**
```python
templates = {
    "bug_needs_info": """
    Thanks for reporting! To help us investigate:
    - What version are you using?
    - Steps to reproduce?
    - Error messages/logs?
    """,
    
    "duplicate": """
    Thanks for reporting! This appears similar to #{duplicate_id}.
    Following that issue for updates.
    """,
    
    "feature_acknowledged": """
    Thanks for the suggestion! We've added this to our backlog.
    Team will review in next planning session.
    """
}
```

---

## üß™ Testing Strategy

### Unit Tests
```python
# Test classification agent
def test_bug_classification():
    issue = {"title": "App crashes", "body": "Stack trace..."}
    result = classifier_agent.classify(issue)
    assert result.category == "bug"
    assert result.confidence > 0.8

# Test duplicate detection
def test_duplicate_detection():
    issue1 = create_test_issue("Memory leak in worker")
    issue2 = create_test_issue("Worker consuming too much memory")
    similarity = similarity_agent.check_similarity(issue1, issue2)
    assert similarity > 0.85
```

### Integration Tests
```python
# Test full webhook flow
def test_webhook_to_github():
    payload = load_webhook_payload("issue_opened.json")
    response = client.post("/webhook", json=payload)
    assert response.status_code == 200
    
    # Check GitHub API was called
    assert mock_github_api.apply_labels.called
    assert mock_github_api.add_comment.called
```

### Load Tests
```python
# Simulate high issue volume
def test_handle_100_concurrent_issues():
    issues = generate_test_issues(100)
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(process_issue, i) for i in issues]
        results = [f.result() for f in futures]
    
    assert all(r.success for r in results)
    assert max(r.latency for r in results) < 60  # seconds
```

---

## üìà Evaluation Metrics

### Agent Performance

```python
# Classification metrics
{
    "accuracy": 0.87,
    "precision": {"bug": 0.89, "feature": 0.85},
    "recall": {"bug": 0.91, "feature": 0.83},
    "f1_score": 0.87
}

# Duplicate detection
{
    "precision": 0.82,  # % of flagged duplicates that are actual
    "recall": 0.78,     # % of actual duplicates found
    "false_positive_rate": 0.05
}

# Routing accuracy
{
    "correct_assignments": 0.84,  # Human agreement
    "reassignment_rate": 0.12     # % issues reassigned by humans
}
```

### System Performance

```python
# Latency (P50, P95, P99)
{
    "webhook_to_queue": "50ms, 100ms, 200ms",
    "processing_time": "5s, 15s, 30s",
    "github_api_call": "200ms, 500ms, 1s"
}

# Throughput
{
    "issues_per_minute": 20,
    "concurrent_workers": 5
}
```

### Business Impact

```
Before Agent:
- Avg time to first response: 4 hours
- Avg time to triage: 8 hours
- Mislabeled issues: 25%

After Agent:
- Avg time to first response: 30 seconds (-99%)
- Avg time to triage: 2 minutes (-98%)
- Mislabeled issues: 8% (-68%)

ROI:
- Time saved per issue: 8 minutes
- For 100 issues/day: 13+ hours/day saved
- Cost: $50/month (API costs)
- Value: ~$10k/month (engineering time)
```

---

## üí∞ Cost Analysis

### API Costs (Monthly)

```
GPT-3.5-turbo (MVP):
- 100 issues/day √ó 2000 tokens/issue √ó 30 days = 6M tokens
- Cost: $3 (input) + $6 (output) = ~$9/month

GPT-4 (Production):
- Same volume
- Cost: $180 (input) + $540 (output) = ~$720/month

Embeddings (text-embedding-3-small):
- 100 issues/day √ó 500 tokens/issue √ó 30 days = 1.5M tokens
- Cost: $0.02/1M tokens = ~$0.03/month

Total MVP: ~$10/month
Total Production: ~$750/month
```

### Infrastructure Costs

```
Railway/Render:
- Hobby: $5/month (sufficient for MVP)
- Pro: $20/month (production)

PostgreSQL:
- Render: $7/month (1GB)

Redis:
- Upstash: $10/month (free tier works for MVP)

Total Infrastructure: $15-40/month
```

### Cost Optimization Strategies

1. **Use Groq API** (fast, cheap)
   - Llama 3 70B: ~$0.60/1M tokens
   - Reduce costs by 90%

2. **Batch processing**
   - Process non-urgent issues in batches
   - Reduce API calls by 50%

3. **Caching**
   - Cache similar issue responses
   - Reuse embeddings for identical text

4. **Smart routing**
   - Use rules for simple cases (no LLM)
   - LLM only for complex decisions

---

## üöÄ Deployment Strategy

### Development Environment
```yaml
# docker-compose.yml
version: '3.8'
services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/issues
      - REDIS_URL=redis://redis:6379
    depends_on:
      - db
      - redis

  worker:
    build: .
    command: celery -A src.worker worker --loglevel=info
    depends_on:
      - redis
      - db

  db:
    image: postgres:15
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:
```

### Production Deployment (Railway)

```bash
# Deploy to Railway
railway login
railway init
railway add  # Add PostgreSQL, Redis
railway up   # Deploy

# Set environment variables
railway variables set OPENAI_API_KEY=...
railway variables set GITHUB_TOKEN=...
```

### CI/CD Pipeline

```yaml
# .github/workflows/deploy.yml
name: Deploy

on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run tests
        run: |
          pip install -r requirements.txt
          pytest tests/

  deploy:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Deploy to Railway
        run: |
          railway up
        env:
          RAILWAY_TOKEN: ${{ secrets.RAILWAY_TOKEN }}
```

---

## üìö Learning Resources

### LangChain & Agents
- [LangChain Documentation](https://python.langchain.com)
- [Building Multi-Agent Systems](https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/)
- [LangChain Cookbook](https://github.com/langchain-ai/langchain/tree/master/cookbook)

### RAG & Vector Databases
- [ChromaDB Docs](https://docs.trychroma.com)
- [RAG from Scratch](https://github.com/langchain-ai/rag-from-scratch)
- [LlamaIndex Guide](https://docs.llamaindex.ai)

### GitHub API
- [GitHub Webhooks](https://docs.github.com/en/webhooks)
- [REST API Reference](https://docs.github.com/en/rest)
- [PyGithub Library](https://pygithub.readthedocs.io)

### MLOps
- [FastAPI Best Practices](https://github.com/zhanymkanov/fastapi-best-practices)
- [Celery Documentation](https://docs.celeryproject.org)
- [Prometheus Python Client](https://github.com/prometheus/client_python)

---

## üìä Project Timeline

```
Week 1-2: MVP
‚îú‚îÄ Day 1-2: Project setup, GitHub webhook
‚îú‚îÄ Day 3-4: Classification agent
‚îú‚îÄ Day 5-6: Routing & response agents
‚îî‚îÄ Day 7-14: Testing, deployment, polish

Week 3-4: Intelligence Layer
‚îú‚îÄ Day 15-18: Embeddings & similarity search
‚îú‚îÄ Day 19-21: Priority prediction
‚îî‚îÄ Day 22-28: Evaluation & refinement

Week 5-6: Production Features
‚îú‚îÄ Day 29-32: Celery + Redis setup
‚îú‚îÄ Day 33-35: Monitoring & logging
‚îî‚îÄ Day 36-42: Admin dashboard, docs

Week 7-8: Advanced Features (Optional)
‚îú‚îÄ Day 43-46: CrewAI migration
‚îú‚îÄ Day 47-49: RAG integration
‚îî‚îÄ Day 50-56: Code suggestions, polish
```

---

## üéì Portfolio Presentation

### GitHub README Highlights

```markdown
# ü§ñ Intelligent Issue Triaging Agent

> Automated GitHub issue management using multi-agent LLM system

## Impact
- ‚ö° 99% faster initial response (4 hours ‚Üí 30 seconds)
- üéØ 87% classification accuracy
- üí∞ $10k/month value (13+ hours saved daily)
- üîç 82% duplicate detection precision

## Tech Stack
- Multi-agent orchestration (CrewAI)
- Production RAG system (LlamaIndex + ChromaDB)
- Real-time processing (FastAPI + Celery)
- MLOps (Prometheus, Grafana)

## Key Features
1. Intelligent classification & prioritization
2. Duplicate detection via embeddings
3. Smart routing to team members
4. Context-aware auto-responses
5. Knowledge base integration
```

### Resume Bullets

```
ü§ñ Built multi-agent issue triaging system processing 100+ daily GitHub 
   issues with 87% classification accuracy, reducing manual triage time by 98%

üéØ Implemented production RAG pipeline with ChromaDB and LlamaIndex for 
   duplicate detection (82% precision) and context-aware responses

‚ö° Architected scalable async system using FastAPI, Celery, and Redis, 
   handling 20 issues/minute with <30s latency (P99)

üìä Delivered measurable impact: $10k/month value through 13+ hours/day 
   saved, validated through A/B testing and user feedback
```

### Interview Talking Points

1. **Multi-agent coordination**: "I built a system where specialized agents 
   collaborate - one classifies, one detects duplicates, one routes. Used 
   CrewAI for orchestration."

2. **Production ML**: "This isn't just a demo. Handles rate limits, retries, 
   monitoring. Built with production ML best practices from day one."

3. **Business impact**: "Reduced triage time from 8 hours to 2 minutes. 
   For teams with 100 issues/day, that's massive ROI."

4. **Technical depth**: "Implemented semantic similarity using embeddings, 
   built RAG for knowledge base, added predictive analytics for priority."

---

## üéØ Success Criteria

### Phase 1 (MVP) Success
- [ ] Successfully processes webhook events
- [ ] Classifies 100 test issues with >80% accuracy
- [ ] Auto-applies labels via GitHub API
- [ ] Deployed and accessible via URL
- [ ] Basic tests passing

### Phase 2 Success
- [ ] Detects duplicates with >75% precision
- [ ] Priority predictions validated by humans
- [ ] PostgreSQL storing issue history
- [ ] ChromaDB indexing all issues

### Phase 3 Success
- [ ] Handles 20 issues/minute sustained
- [ ] <1 minute P95 latency
- [ ] Monitoring dashboard functional
- [ ] Zero downtime in 1-week test

### Portfolio Success
- [ ] Live demo available
- [ ] Comprehensive README with metrics
- [ ] 3+ blog posts documenting journey
- [ ] Positive feedback from 5+ users
- [ ] Used in job interviews successfully

---

## üöß Potential Challenges & Solutions

### Challenge 1: GitHub API Rate Limits
**Problem:** 5000 requests/hour for authenticated users
**Solution:** 
- Implement request queuing
- Batch operations where possible
- Use conditional requests (ETag)
- Add exponential backoff

### Challenge 2: LLM Hallucinations
**Problem:** GPT-4 might assign wrong labels/teams
**Solution:**
- Use structured outputs (function calling)
- Confidence thresholds (manual review if <0.7)
- Human-in-the-loop for critical decisions
- Regular evaluation against ground truth

### Challenge 3: Cold Start (New Repos)
**Problem:** No historical data for routing/priority
**Solution:**
- Start with rule-based defaults
- Use transfer learning from similar projects
- Active learning (ask maintainers to label first 50)
- Gradual confidence building

### Challenge 4: Cost Management
**Problem:** GPT-4 costs add up quickly
**Solution:**
- Use GPT-3.5 for simple tasks
- Migrate to Llama 3 via Groq
- Implement caching aggressively
- Use rules for obvious cases

---

## üîÑ Iteration Strategy

### Week 1-2: Build & Break
- Build MVP quickly, don't over-engineer
- Test with fake data first
- Deploy early, iterate based on real usage

### Week 3-4: Measure & Learn
- Instrument everything (metrics, logs)
- Talk to users, gather feedback
- Identify biggest pain points
- Iterate on most impactful features

### Week 5-6: Polish & Scale
- Fix bugs found in real usage
- Optimize bottlenecks
- Add missing error handling
- Improve documentation

### Week 7-8: Showcase & Share
- Write technical blog posts
- Create demo videos
- Share on Twitter/LinkedIn
- Present in communities

---

## üìù Next Steps

### Immediate Actions (Week 1)
1. [ ] Clone starter template
2. [ ] Set up GitHub webhook test repo
3. [ ] Get OpenAI API key
4. [ ] Create project structure
5. [ ] Implement first agent (classifier)

### Repository Setup
```bash
# Initialize project
git init issue-triaging-agent
cd issue-triaging-agent

# Create structure
mkdir -p src/{agents,api,models,utils,embeddings,ml}
mkdir -p tests/{unit,integration}
touch README.md requirements.txt docker-compose.yml

# Install dependencies
pip install fastapi uvicorn langchain openai chromadb \
    celery redis sqlalchemy pydantic pytest

# Start building!
```

### First Commit Goals
- [ ] FastAPI server responds to health check
- [ ] Webhook endpoint receives GitHub events
- [ ] Basic classification agent works
- [ ] Tests for classification agent pass

---

## üéâ Final Thoughts

This project hits the sweet spot for your portfolio:

‚úÖ **Production ML**: RAG, embeddings, multi-agent systems
‚úÖ **Real Impact**: Measurable time/cost savings
‚úÖ **Technical Depth**: MLOps, async processing, monitoring
‚úÖ **Practical Problem**: Every company with GitHub needs this
‚úÖ **Scalable Architecture**: Shows system design thinking

**Target completion:** 6-8 weeks (MVP in 2 weeks!)

**Expected outcome:** 
- Impressive portfolio piece
- Deep understanding of agent systems
- Production ML experience
- Conversation starter in interviews

Ready to start building? Begin with Phase 1, Week 1! üöÄ